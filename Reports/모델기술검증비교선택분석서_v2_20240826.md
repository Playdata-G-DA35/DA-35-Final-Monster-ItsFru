# Instance Segmentation 기술 검증 비교 선택 분석서

## 1. 개요

본 프로젝트에서는 인스턴스 세그멘테이션(Instance Segmentation)기술을 활용해 과일의 품목을 분류하고, 객체의 마스크를 추출해 이상 부분을 탐지하고자 한다. 따라서 높은 정확도와 빠른 추론 속도로 객체를 탐지하고 마스크를 추출하는 Instance Segmentation 모델을 선택하고, custom dataset으로 파인튜닝하여 과일 객체를 탐지하고 분류하는 segmentation 모델을 만들고자 한다. 

## 2. 사용 기술 

- 세그멘테이션(Segmentation) : 영상에서 객체를 식별하고, 픽셀단위로 특정 클래스에 할당하여 객체의 정확한 형태를 식별하는 기술
   - 시맨틱 세그멘테이션(Semantic Segmentation) : 같은 클래스의 다른 객체를 구분하지 않음
   - 인스턴스 세그멘테이션(Instance Segmentation) : 같은 클래스에서도 서로 다른 객체를 구분하는 것이 특징
- 각 과일의 개별적인 인식과 정확한 수량 파악을 위해 인스턴스 세그멘테이션 모델 활용

## 3. 모델 비교 및 선정

### 3.1. COCO Benchmark 결과 비교 

- 세그멘테이션 모델은 객체탐지 모델을 기반으로 힘
- 객체 탐지 모델과 함께 COCO 벤치마크 결과를 비교하여 모델의 전반적인 성능을 파악하고 적합한 인스턴스 세그멘테이션 모델을 선택하고자 함 
   - MS COCO 데이터셋 : 객체 탐지와 세그멘테이션 모델의 성능을 평가하는 데 사용되는 대표적인 데이터셋 


| Model         | mAP (0.5:0.95) | mAP@0.5 | mAP@0.75 | Params (M) | FPS (on GPU) | Year | Reference                       |
|---------------|----------------|---------|----------|------------|--------------|------|---------------------------------|
| Faster R-CNN  | 37.4           | 58.4    | 39.4     | 41         | ~6-7         | 2015 | 논문: [Faster R-CNN](https://arxiv.org/abs/1506.01497) |
| Mask R-CNN    | 38.2           | 60.3    | 40.6     | 44         | ~5           | 2017 | 논문: [Mask R-CNN](https://arxiv.org/abs/1703.06870)   |
| SSD           | 25.1           | 43.1    | 25.8     | 34         | ~46          | 2016 | 논문: [SSD](https://arxiv.org/abs/1512.02325)          |
| YOLOv5        | 50.4           | 69.7    | 55.2     | 7.1        | ~140         | 2020 | GitHub: [YOLOv5 GitHub](https://github.com/ultralytics/yolov5) |
| YOLOv8        | 52.3           | 70.6    | 56.3     | 11.2       | ~150         | 2023 | GitHub: [YOLOv8 GitHub](https://github.com/ultralytics/yolov8) |

> **성능 평가 지표**
> - mAP (0.5:0.95): IoU 0.5에서 0.95까지를 기준으로 한 평균 정밀도
>   - IoU(Intersection over Union) : 객체탐지 모델이 예측한 bbox와 실제 bbox 간의 일치도 측정 지표
> - mAP@0.5: IoU 0.5에서의 평균 정밀도
>- mAP@0.75:  IoU  0.75에서의 평균 정밀도
>- Params (M):  모델의 파라미터 수(백만 단위), 모델의 복잡도와 메모리 요구 사항 의미
> - FPS (on GPU): GPU에서의 초당 프레임 수

### 3.2. Instance Segmentation 모델 비교 
#### -  YOLOv8-seg vs. Mask R-CNN

- COCO 벤치마크 모델 중 세그멘테이션 모델
- YOLOv8-seg : 단일 스테이지 접근 방식, 객체탐지와 세그멘테이션이 동시에 이루어져 처리 속도가 빠르고 실시간 성능에 강점
- Mask R-CNN : 두 스테이지 접근 방식, 첫 번째 스테이지에서 Region Proposal을 생성하고, 두 번째 스테이지에서는 제안된 영역에서 객체 분류 및 마스크를 예측해 정확도가 높고 세부적인 객체 처리에 유리
- 본 프로젝트에서는 고객이 촬영한 사진에 대해 실시간 추론이 이루어질 수 있도록 YOLOv8-seg모델을 선택

| **비교 항목**                | **YOLOv8-seg**                              | **Mask R-CNN**                        |
|-----------------------------|---------------------------------------------|---------------------------------------|
| **기반 아키텍처**            | YOLO (You Only Look Once)                   | Faster R-CNN (Region Proposal 기반)   |
| **접근 방식**                | 단일 스테이지 접근                          | 두 스테이지 접근                      |
| **세그멘테이션 방식**        | Bounding Box와 마스크 예측을 동시에 수행    | Region Proposal 후 마스크 예측        |
| **학습 속도**                | 빠름                                       | 느림                                 |
| **추론 속도**                | 실시간 성능 가능                           | 상대적으로 느림                       |
| **모델 복잡도**              | 상대적으로 단순함                          | 상대적으로 복잡함                     |
| **메모리 사용량**            | 낮음                                       | 높음                                 |
| **성능 (COCO mAP)**          | 중간 (정확도는 다소 낮음)                  | 높음 (정확도가 우수함)               |
| **처리 가능한 객체 크기**    | 주로 큰 객체에 유리                        | 작은 객체도 잘 처리 가능              |


## 4. YOLO 프레임워크 특징 

1. 객체 탐지 작업을 단일 네트워크(CNN)에서 처리하여 처리 속도가 빠르고, 실시간 추론이 가능
2. 예측된 객체의 마스크, 바운딩 박스, 클래스 확률을 출력하여 간단한 후처리 가능
3. 다양한 버전(Object Detection, Segmentation, Pose)과 사이즈(n, s, m, l, x)의 모델을 제공해 확장 가능성이 높음

### 4.1. YOLOv8 아키텍처
#### - 모델 아키텍처 Overview
- Backbone: YOLOv8 모델의 공통적인 특징 추출 부분
- Head: 각 작업(Detection, Segmentation, Pose Estimation)에 따라 다르게 설계
   - Detect: 객체 탐지에 특화된 헤드
   - Segmentation: 이미지 분할에 특화된 헤드
   - Pose Estimation: 사람의 자세 추정에 특화된 헤드
     
![yolov8_architecture](https://github.com/user-attachments/assets/fcc4924d-cf60-4f28-954c-5750868aea09)


#### - Backbone 

- CNN 기반 특징 추출 구조로, 다양한 레이어와 블록을 사용해 이미지의 특징을 점진적으로 추출
- 구성 요소:
   - 피라미드 구조: FPN(Feature Pyramid Network)와 PAN(Path Aggregation Network)을 결합해 여러 계층에서 추출된 피처를 결합, 다양한 스케일의 정보를 수집하고 다양한 크기의 객체를 인식
   - Conv: 특징을 추출하는 기본 합성곱 레이어로, 이미지의 저수준 특징을 추출
   - C2f (Cross Stage Partial with Focus): 중간 피처를 결합해 정보 전파를 효율적으로 수행, CSPNet(Cross Stage Partial Network)의 개선된 형태로, 네트워크의 효율성을 높이면서 정보 손실을 최소화
   - SPPF (Spatial Pyramid Pooling - Fast): 다양한 크기의 피처를 결합해 전역 특징을 추출하는 블록, SPP(Spatial Pyramid Pooling)를 개선해 속도와 메모리 효율성을 향상
#### - Head
- 각 작업에 따라 특화된 구조를 가짐
  - Detect Head: 이미지에서 객체 위치(바운딩 박스)와 클래스(종류)를 예측, 빠르고 정확한 객체 탐지 수행
  - Segmentation Head: 이미지의 각 픽셀이 어떤 객체에 속하는지를 예측해 객체의 모양을 정확하게 분할
  - Pose Estimation Head: 사람의 관절 위치를 예측해 자세를 추정

### 4.2. YOLOv8-segmentation 모델

- YOLOv8 모델의 사이즈별 COCO 벤치마크 결과
  
| MODEL       | SIZE | mAPbox 50-95 | mAPmask 50-95 | Speed CPU ONNX (ms) | Speed A100 TensorRT (ms) | Params (M) | FLOPs (B) |
|-------------|------|--------------|---------------|----------------------|---------------------------|------------|-----------|
| YOLOv8n-seg | 640  | 36.7         | 30.5          | 96.1                 | 1.21                      | 3.4        | 12.6      |
| YOLOv8s-seg | 640  | 44.6         | 36.8          | 155.7                | 1.47                      | 11.8       | 42.6      |
| YOLOv8m-seg | 640  | 49.9         | 40.8          | 317.0                | 2.18                      | 27.3       | 110.2     |
| YOLOv8l-seg | 640  | 52.3         | 42.6          | 572.4                | 2.79                      | 46.0       | 220.5     |
| YOLOv8x-seg | 640  | 53.4         | 43.4          | 712.1                | 4.02                      | 71.8       | 344.1     |

> **성능 평가 지표** 
> - mAPbox : 객체 탐지 평균 정밀도(mAP)
> - mAPmask : 세그멘테이션 평균 정밀도(mAP)
> - Speed CPU ONNX(ms) : CPU에서 onnx형식으로 모델 실행시 속도
> - Speed A100 TensorRT(ms) : NVIDIA A100 GPU에서 TensorRT로 모델을 실행할 때의 속도
> - Params (M) : 모델의 총 파라미터 수, 모델의 복잡성 및 용량 측정
> - FLOPs (B) : 모델이 처리하는 부동 소수점 연산 수

### 5. 파인튜닝 계획
1. 데이터 
   - AIHUB 농산물 품질 (QC) 데이터셋
   - AIHUB Ego-Vision 관점의 2D, 3D 손 움직임 데이터
   - 직접 촬영한 객체 상호작용 손 데이터 

2. 활용 모델
   - 하이퍼파라미터 튜닝 시 YOLOv8n-seg을 활용
   - 최적의 하이퍼파라미터 조합으로 초기 설정 후 YOLOv8m-seg 모델로 최종 학습

3. 학습 환경 
   
| 항목       | 1차 학습        | 2차 학습               |
|------------|-----------------|------------------------|
| IDE        | Google Colab    | Google Colab           |
| GPU 사용량 | Tesla T4        | NVIDIA A100-SXM4-40GB  |
| GPU 메모리 | 15GB            | 40GB                   |

