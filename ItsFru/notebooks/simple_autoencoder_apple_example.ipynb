{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"L4","mount_file_id":"1x3aEannW5kqWFpDBbrMdpF3OmzHE4ghv","authorship_tag":"ABX9TyM/6vpi4vjeNiWK0OhCfHdD"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["import os\n","import cv2\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Dataset\n","from torchvision import transforms\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n"],"metadata":{"id":"M4QhJBxOFIOL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# LAB 색 공간의 평균과 표준편차\n","lab_mean = [0.485, 0.456, 0.406]\n","lab_std = [0.229, 0.224, 0.225]\n","\n","class AppleDataset(Dataset):\n","    def __init__(self, image_dir, transform=None):\n","        self.image_dir = image_dir\n","        self.image_paths = [os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.endswith('.jpg')]\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.image_paths)\n","\n","    def __getitem__(self, idx):\n","        img_path = self.image_paths[idx]\n","        image = cv2.imread(img_path)\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n","        image = cv2.resize(image, (256, 256))\n","        if self.transform:\n","            image = self.transform(image)\n","        return image\n","\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=lab_mean, std=lab_std)\n","])\n","\n","train_dataset = AppleDataset(image_dir='/content/drive/MyDrive/aet/apple', transform=transform)\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n"],"metadata":{"id":"NPXCvDwqGpHN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class ResidualBlock(nn.Module):\n","    def __init__(self, in_channels):\n","        super(ResidualBlock, self).__init__()\n","        self.conv1 = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1)\n","        self.bn1 = nn.BatchNorm2d(in_channels)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.conv2 = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1)\n","        self.bn2 = nn.BatchNorm2d(in_channels)\n","\n","    def forward(self, x):\n","        residual = x\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","        out += residual\n","        out = self.relu(out)\n","        return out\n","\n","class ImprovedAutoencoder(nn.Module):\n","    def __init__(self):\n","        super(ImprovedAutoencoder, self).__init__()\n","\n","        # Encoder\n","        self.encoder = nn.Sequential(\n","            nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(inplace=True),\n","            ResidualBlock(64),\n","            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(inplace=True),\n","            ResidualBlock(128),\n","            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(inplace=True),\n","            ResidualBlock(256),\n","            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n","            nn.BatchNorm2d(512),\n","            nn.ReLU(inplace=True),\n","            ResidualBlock(512)\n","        )\n","\n","        # Decoder\n","        self.decoder = nn.Sequential(\n","            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(inplace=True),\n","            ResidualBlock(256),\n","            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(inplace=True),\n","            ResidualBlock(128),\n","            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(inplace=True),\n","            ResidualBlock(64),\n","            nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1),\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, x):\n","        encoded = self.encoder(x)\n","        decoded = self.decoder(encoded)\n","        return decoded"],"metadata":{"id":"Z_AvawcYGrsR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","model = ImprovedAutoencoder().cuda()\n","criterion = nn.MSELoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","num_epochs = 100\n","for epoch in range(num_epochs):\n","    model.train()\n","    for images in train_loader:\n","        images = images.cuda()\n","        outputs = model(images)\n","        loss = criterion(outputs, images)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Vy_7Wc9-Gs2D","executionInfo":{"status":"ok","timestamp":1721982890730,"user_tz":-540,"elapsed":69785,"user":{"displayName":"seohee lee","userId":"08000749035686676726"}},"outputId":"88b1ab8a-a3dd-4ba2-cafa-0f8f57d4d3f5","collapsed":true},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [1/100], Loss: 0.6221\n","Epoch [2/100], Loss: 0.5531\n","Epoch [3/100], Loss: 0.5069\n","Epoch [4/100], Loss: 0.4835\n","Epoch [5/100], Loss: 0.4645\n","Epoch [6/100], Loss: 0.4498\n","Epoch [7/100], Loss: 0.4381\n","Epoch [8/100], Loss: 0.4276\n","Epoch [9/100], Loss: 0.4180\n","Epoch [10/100], Loss: 0.4104\n","Epoch [11/100], Loss: 0.3972\n","Epoch [12/100], Loss: 0.3866\n","Epoch [13/100], Loss: 0.3787\n","Epoch [14/100], Loss: 0.3708\n","Epoch [15/100], Loss: 0.3623\n","Epoch [16/100], Loss: 0.3527\n","Epoch [17/100], Loss: 0.3446\n","Epoch [18/100], Loss: 0.3283\n","Epoch [19/100], Loss: 0.3163\n","Epoch [20/100], Loss: 0.3181\n","Epoch [21/100], Loss: 0.2962\n","Epoch [22/100], Loss: 0.2760\n","Epoch [23/100], Loss: 0.2636\n","Epoch [24/100], Loss: 0.2508\n","Epoch [25/100], Loss: 0.2457\n","Epoch [26/100], Loss: 0.2375\n","Epoch [27/100], Loss: 0.2337\n","Epoch [28/100], Loss: 0.2292\n","Epoch [29/100], Loss: 0.2252\n","Epoch [30/100], Loss: 0.2210\n","Epoch [31/100], Loss: 0.2196\n","Epoch [32/100], Loss: 0.2174\n","Epoch [33/100], Loss: 0.2146\n","Epoch [34/100], Loss: 0.2122\n","Epoch [35/100], Loss: 0.2110\n","Epoch [36/100], Loss: 0.2093\n","Epoch [37/100], Loss: 0.2076\n","Epoch [38/100], Loss: 0.2057\n","Epoch [39/100], Loss: 0.2041\n","Epoch [40/100], Loss: 0.2020\n","Epoch [41/100], Loss: 0.2007\n","Epoch [42/100], Loss: 0.1984\n","Epoch [43/100], Loss: 0.1971\n","Epoch [44/100], Loss: 0.1948\n","Epoch [45/100], Loss: 0.1934\n","Epoch [46/100], Loss: 0.1926\n","Epoch [47/100], Loss: 0.1912\n","Epoch [48/100], Loss: 0.1903\n","Epoch [49/100], Loss: 0.1893\n","Epoch [50/100], Loss: 0.1880\n","Epoch [51/100], Loss: 0.1871\n","Epoch [52/100], Loss: 0.1859\n","Epoch [53/100], Loss: 0.1850\n","Epoch [54/100], Loss: 0.1844\n","Epoch [55/100], Loss: 0.1834\n","Epoch [56/100], Loss: 0.1823\n","Epoch [57/100], Loss: 0.1814\n","Epoch [58/100], Loss: 0.1808\n","Epoch [59/100], Loss: 0.1803\n","Epoch [60/100], Loss: 0.1795\n","Epoch [61/100], Loss: 0.1787\n","Epoch [62/100], Loss: 0.1781\n","Epoch [63/100], Loss: 0.1778\n","Epoch [64/100], Loss: 0.1773\n","Epoch [65/100], Loss: 0.1769\n","Epoch [66/100], Loss: 0.1764\n","Epoch [67/100], Loss: 0.1760\n","Epoch [68/100], Loss: 0.1756\n","Epoch [69/100], Loss: 0.1752\n","Epoch [70/100], Loss: 0.1747\n","Epoch [71/100], Loss: 0.1742\n","Epoch [72/100], Loss: 0.1738\n","Epoch [73/100], Loss: 0.1734\n","Epoch [74/100], Loss: 0.1732\n","Epoch [75/100], Loss: 0.1736\n","Epoch [76/100], Loss: 0.1741\n","Epoch [77/100], Loss: 0.1729\n","Epoch [78/100], Loss: 0.1711\n","Epoch [79/100], Loss: 0.1712\n","Epoch [80/100], Loss: 0.1721\n","Epoch [81/100], Loss: 0.1714\n","Epoch [82/100], Loss: 0.1702\n","Epoch [83/100], Loss: 0.1705\n","Epoch [84/100], Loss: 0.1696\n","Epoch [85/100], Loss: 0.1688\n","Epoch [86/100], Loss: 0.1694\n","Epoch [87/100], Loss: 0.1680\n","Epoch [88/100], Loss: 0.1684\n","Epoch [89/100], Loss: 0.1678\n","Epoch [90/100], Loss: 0.1674\n","Epoch [91/100], Loss: 0.1673\n","Epoch [92/100], Loss: 0.1669\n","Epoch [93/100], Loss: 0.1665\n","Epoch [94/100], Loss: 0.1665\n","Epoch [95/100], Loss: 0.1660\n","Epoch [96/100], Loss: 0.1659\n","Epoch [97/100], Loss: 0.1658\n","Epoch [98/100], Loss: 0.1657\n","Epoch [99/100], Loss: 0.1652\n","Epoch [100/100], Loss: 0.1652\n"]}]},{"cell_type":"code","source":["\n","def denormalize(tensor, mean, std):\n","    for t, m, s in zip(tensor, mean, std):\n","        t.mul_(s).add_(m)\n","    return tensor\n","\n","def detect_anomalies_and_visualize_heatmap(model, image_dir, threshold=0.01):\n","    model.eval()\n","    for img_name in os.listdir(image_dir):\n","        if img_name.endswith('.jpg'):\n","            img_path = os.path.join(image_dir, img_name)\n","            image = cv2.imread(img_path)\n","            image = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n","            image = cv2.resize(image, (256, 256))\n","            image_tensor = transform(image).unsqueeze(0).cuda()\n","\n","            with torch.no_grad():\n","                reconstructed = model(image_tensor)\n","                loss = criterion(reconstructed, image_tensor)\n","                if loss.item() > threshold:\n","                    print(f'Anomaly detected in image: {img_name}, Loss: {loss.item():.4f}')\n","\n","                    reconstructed_image = reconstructed.squeeze(0).cpu()\n","                    original_image = image_tensor.squeeze(0).cpu()\n","\n","                    reconstructed_image = denormalize(reconstructed_image, lab_mean, lab_std).numpy().transpose(1, 2, 0)\n","                    original_image = denormalize(original_image, lab_mean, lab_std).numpy().transpose(1, 2, 0)\n","\n","                    original_image = (original_image * 255).astype(np.uint8)\n","                    reconstructed_image = (reconstructed_image * 255).astype(np.uint8)\n","\n","                    original_image_rgb = cv2.cvtColor(original_image, cv2.COLOR_LAB2RGB)\n","                    reconstructed_image_rgb = cv2.cvtColor(reconstructed_image, cv2.COLOR_LAB2RGB)\n","\n","                    reconstruction_error = np.abs(original_image - reconstructed_image)\n","                    error_map = np.mean(reconstruction_error, axis=2)\n","\n","                    plt.figure(figsize=(18, 6))\n","                    plt.subplot(1, 3, 1)\n","                    plt.imshow(original_image_rgb)\n","                    plt.title('Original Image')\n","\n","                    plt.subplot(1, 3, 2)\n","                    plt.imshow(reconstructed_image_rgb)\n","                    plt.title('Reconstructed Image')\n","\n","                    plt.subplot(1, 3, 3)\n","                    sns.heatmap(error_map, cmap='viridis')\n","                    plt.title('Reconstruction Error Heatmap')\n","\n","                    plt.show()\n"],"metadata":{"id":"hDAf9vg7GuDJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","detect_anomalies_and_visualize_heatmap(model, '/content/drive/MyDrive/aet/apple_d')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1gfxMpD9V-ewGBLGCaliKyia00434zA36"},"id":"mxnGHEDEGu2B","executionInfo":{"status":"ok","timestamp":1721982249849,"user_tz":-540,"elapsed":9337,"user":{"displayName":"seohee lee","userId":"08000749035686676726"}},"outputId":"83a7038e-390e-4a1a-d229-e965c22ba8f4"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","source":["def preprocess_image(image_path):\n","    image = cv2.imread(image_path)\n","    image = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n","    image = cv2.resize(image, (256, 256))\n","    image_tensor = transform(image).unsqueeze(0).cuda()\n","    return image_tensor\n","\n","def calculate_reconstruction_error(model, image_tensor):\n","    model.eval()\n","    with torch.no_grad():\n","        reconstructed = model(image_tensor)\n","        loss = criterion(reconstructed, image_tensor)\n","        reconstruction_error = torch.abs(image_tensor - reconstructed).cpu().numpy()\n","    return reconstruction_error\n","\n","def calculate_grid_errors(reconstruction_error, grid_size=32):\n","    # 이미지 크기와 그리드 크기를 기반으로 그리드 수 계산\n","    img_size = reconstruction_error.shape[-2]\n","    num_grids = img_size // grid_size\n","\n","    # 그리드별 재구성 오류 평균 계산\n","    grid_errors = np.zeros((num_grids, num_grids))\n","    for i in range(num_grids):\n","        for j in range(num_grids):\n","            grid = reconstruction_error[:, :, i*grid_size:(i+1)*grid_size, j*grid_size:(j+1)*grid_size]\n","            grid_errors[i, j] = np.mean(grid)\n","\n","    return grid_errors\n","\n","def main(image_path):\n","    # 이미지 전처리\n","    image_tensor = preprocess_image(image_path)\n","\n","    # 재구성 오류 계산\n","    reconstruction_error = calculate_reconstruction_error(model, image_tensor)\n","\n","    # 그리드별 재구성 오류 평균 계산\n","    grid_errors = calculate_grid_errors(reconstruction_error)\n","\n","    return grid_errors"],"metadata":{"id":"8kS9cxH1GW7f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 예제 이미지 경로\n","image_path = '/content/drive/MyDrive/aet/apple_d/1.jpg'\n","\n","# 그리드별 재구성 오류 평균 계산\n","grid_errors = main(image_path)\n","print(grid_errors)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BfEqkG8OMGEe","executionInfo":{"status":"ok","timestamp":1721983109866,"user_tz":-540,"elapsed":2265,"user":{"displayName":"seohee lee","userId":"08000749035686676726"}},"outputId":"9f1bb475-53e3-4960-d74e-e9789f578faf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0.43899611 0.34221041 0.28271416 0.33106139 0.31633359 0.25639081\n","  0.33745381 0.41102383]\n"," [0.32531136 0.26268396 0.24896197 0.36678827 0.31475291 0.21755044\n","  0.17883919 0.32101327]\n"," [0.25493965 0.2753531  0.27632645 0.26044083 0.27489141 0.26071429\n","  0.27822918 0.22903883]\n"," [0.23702605 0.30346826 0.25868067 0.27108127 0.26246473 0.24997322\n","  0.25375375 0.19978721]\n"," [0.2344483  0.2836     0.31607214 0.32422081 0.32485929 0.26121745\n","  0.29733595 0.18308286]\n"," [0.30211014 0.23036294 0.3235437  0.34191474 0.27281979 0.18649103\n","  0.251277   0.22659469]\n"," [0.34630856 0.21630366 0.2882711  0.30055165 0.19955151 0.19971411\n","  0.17909354 0.19255282]\n"," [0.28574589 0.20141335 0.17005186 0.26548681 0.24592817 0.20620064\n","  0.14684455 0.1153819 ]]\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"jrxCtASjMPqY"},"execution_count":null,"outputs":[]}]}
